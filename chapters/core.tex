\chapter{Implementation}
\label{cha:core} In this chapter, there is the discussion of the contribution
made in this thesis in the project code, with the goal of switching the language
pipeline from the DSL to BIR.

We start from the first step of the pipeline in \cref{cha:Parsing BIR}, where
there is the discussion of the implementation of the parser for the BIR language,
how it has been done, the instruments and the outcome. We then check the correctness
of the produced parser in the following \cref{cha:Validating the parsing}, where
there is the in depth explanation of all the developed tests and methodologies
to validate the parser validity and robustness. In next \cref{cha:Tracing the
parsed BIR}, there is the development of the next step in the pipeline, the tracer,
the component that populates the values seen in the CPU, and is used to then populate
the contract. To do so, it needs to interpret the parsed BIR language and take trace
of the computed values. All the tests and validation for this component are then
discussed in \cref{cha:Validating the tracing}. Here there is be a really in-depth
discussion of all the test's types and the motivations that brought us to write them.
In \cref{cha:Generating the Rosette code}, there is the discussion of the development
of the grammar in the Rosette language, allowing therefore the generation of new
constraints for the contract, closing the loop of the project. In
\cref{cha:Validating the pipeline}, there is the discussion of the validation of
the whole developed pipelines, with some example of the tests. In following \cref{cha:Code
migration and documentation}, there is a brief discussion of the migration of
the code from the DSL to BIR, the merge operation and the documentation created for
the developers of the project, to allow them to understand the new pipeline and
contribution.

\section{Parsing BIR}
\label{cha:Parsing BIR} The first step in the pipeline is parsing the BIR
language. We start from a raw string given as input, and we first want to
determine whether this string is valid BIR code, and if it is the case, parse it
into some readable and consumable data structure that we can use in following
steps. In order to archive this goal, we first needed to have in mind a clear
idea of the semantic of the language, the grammar and the regex that it is generated
from. In order to do so, we read with the paper that defines the language (\cite{bir_pub}),
and had various chats with the researchers who are in charge of the project, with
specific focus on the BIR language. Some of these person were the researchers
that actually developed BIR in the first place, so it was the perfect environment
for this research. With them, we understood the grammar, the main features and
constructs of the language, and we began to be familiar with the semantic. We
wrote some examples of the code in the \cref{cha:BIR}. We decided so, that we
needed all the basics expressions of the language, so the if then else, the
binary operations, the memory operations, the unary operations, the predicates,
constant values and variables. After some time, we also decided to add the
OPERAND operators, since they became needed for the project as a whole.

Once we had this clear idea of the language in mind, it was time to start
developing the new parser. we did it following the same approach used to developed
the previous one, but adapting the code to the specificities of BIR. We wrote the
parser using the Python library `pyparsing' (\cite{pyparsing}). It is a Parsing
Expression Grammar (PEG) parser, meaning that it is able to parse given strings starting
from a regular expression (or more than one) generated by the developer. This
library is really powerful, and it is able to parse complex languages, with a
lot of features. It is also quite easy to use, and with good documentation. It has
different built-in functions that allows to create powerful regex, such as the
option for creating object that appears `OneOrMore' times, or `ZeroOrMore' times,
and so on. This allows the developer to focus more on the regex of the language itself,
without the need to focus on complex details about the implementation of the
parsing. Creating a new parser completely from scratch would have been a story of
its own, as we also see in the \cref{cha:Migration to Rust}.

Then, to develop this parser, we dived deep into the language and created all
the regular expressions needed to represent the full specification of the
language itself. Since it can generate quite long string, a lot of the code was
split in different sub-functions, objects and routines in the file, to allow
better code reuse and less confusion for future readers and developers. Thanks to
the possibilities that `pyparsing' offers, we also implemented some static
checks in place, so that there was a higher level of robustness in the parser.
An example is the check for the types of operand expected, if they were effectively
number that could be parsed or if the syntax used to describe the regiters was right,
and the register code number actually existed.All of this to have the fully
fledged parser functionalities.

Once the grammar was fully created, we created the class structure that `pyparsing'
needs to put all the parsed string, so we can after the parsing also trace and
evaluate the expression. This class can be seen as a parsing tree, from a high
level. We created this class, that according to the generated keyword, it would have
some specific attributes that is used in future steps of the pipeline. It
behaves as a generic type, or a sort of enum, according to the specific
construct it is modeling. This approach was not quite optimal, you cannot infer
directly what type of expression is being represented, unless you check the used
attributes. So, we added the `keyword' attribute, to make clear and easier to
understand what type of expression was used and allow for more readable code. Other
languages different from Python, such as Rust, can do this with generics or enums,
that are features not offered by Python. This is one of the reasons that made us
thinking about switching the language of this thesis's project to Rust, as we
see in the \cref{cha:Migration to Rust}.

We called this class `BEXP', as Binary Expression. This class is istantiated
when the parser finds a binary expression, and it is used to the eventual operands,
or operation types. Also, this section of the code has been modularized as much
as possible, so in case there is the need to add new constructs or features it should
be as easy as possible.

We did this the first time with all the specification created in the first meetings,
and then added when the necessity arose, also the part for the parsing of the OPERAND
operators. The modularity made it easier than expected to add also the newly
added requirements, allowing for faster development times. In the end, the artifact
for this step was a Python file with multiple classes and functions, that allows
the parsing of all valid BIR strings, and the creation of a `BEXP' object according
to the parsing string content. In case this is not possible, due to some errors,
it throws an exception error, with the character that caused the error, since
not expected. `pyparsing' showed to be extremely powerful, very user friendly, and
fast both in terms of runtime execution, but also in writing time needed, debugging
and learning curve. And since it was already used from the other members of the
team, it does not introduce complexity or technical debt of any kind, so in
future can be maintained and updated with ease, if we also consider the modularity
added by design.

To introduce the new parser in the codebase, it was enough to add a if statement
at boot time, that accoring to the configuration used to run the project, if the
selected language was `bir', then the used parser would have been `BIRParser' in
place of the legacy `Parser'.

\section{Validating the parsing}
\label{cha:Validating the parsing} Then it was time for validating the parsing.
Since this is the first and arguably most fundamental step in the pipeline, it needed
a quite big and comprehensive test suite, to certify its correctness. In order to
archive this, for each possible construct of the language, we wrote different test
cases. They could be tests where a valid string was parsed, and we checked that the
class instance output contained in fact what we wanted and expected from it.
This was effectively a visit of the parse tree generated, inspecting all the values
and checking their correctness. Another type of test that we implemented was the
creation of invalid BIR string, and we expected a failure from the parser. If
that would not happen, then there was some issue in the current implementation.

For all the testing, also in \cref{cha:Validating the tracing}, we used the Python
library `unittest'. This library allows to create a `TestCase' class instance, in
which developers can define their test cases. The class allows for a `setUp'
method that is executed before the whole test case suite is. Then, all the
methods that has a function name starting with `test' are executed. In the function,
we inserted many `assert' statements, to check that the outcome of the executed function
was what we expected. If the test runs as expected and all the assertions are
met, the the test is counted as passed, otherwise it fails. In the end of the simulation,
the library also provides a brief summary of all the executed tests, how many passed
and the error that caused the failure of the failed ones. Also this library has
proven to be extremely powerful and capable. Making it easy to write the test suite.
In fact, this library has been used also in all the testing done in this thesis,
but is also the standard the facto for testing any kind of Python code, both in the
word of research and in industry. Therefore, the choice was quite obvious and easy.
To then, in the end be able to run all the generate tests, we also wrote a brief
shell script, which executed all the tests and made some prettier printing of the
results.

Doing this part has been extremely useful, since it allowed to check that the
current implementation was correct, and also in the meantime, while correcting
the errors, be sure that we did not broke any working piece of the software. The
last part of the implementation has been done alongside the testing, since this improved
the speed and productivity of code writing and effective artifacts produced.
This allows in the future, to do some addition to the parser, and test
automatically in a matter of milliseconds if the implemented change did not
break anything. This gave us also a chance to approach Test-Driven Development (TDD),
that is a developing methodology quite popular, and understand the benefits it provides.

\section{Tracing the parsed BIR}
\label{cha:Tracing the parsed BIR} The next step in the pipeline was tracing the
parsed BIR code. This means that we needed to create a tracer, a piece of software
that, given the contract written in BIR and the state of the CPU, called `model'
in this project, it would be able to populate the trace. The trace is a list data
structure that contains all the information that are evaluated from the CPU,
according to its contract. So, if we ask with a BIR contract to evaluate a
conditional statement (if then else) with a true condition, and then accordingly
return the value in the register RAX or RBX, we expect to find in the trace the
value contained in the RAX register.

The step of tracing is really important, because this allow us to check what information
are leaked by the contract. Then, we are able to infer the knowledge, and add
more constraint to the CPU contract, and then check loop over again, up to we
reach a fixed point, as detailed in \cref{cha: Loop outlook}.

To accomplish this goal of tracing the contracts, populating the trace and
allowing the project to run his loop, we needed to create a new class for this purpose.
To maintain the code as close as his original state, and not bring too much complexity,
we followed the same approach used for the legacy tracer for the DSL. we created
the class `BIRParametricTracer', that is a subclass of `UnicornTracer'. The `UnicornTracer'
class is a defined class in the codebase of the Malcos project, and it is a
subclass of `ABC' class, that is a Python standard abstract class. The `UnicornTracer'
class is used therefore as an interface inside the codebase, to allow for the creation
of different tracers, and all has a trace, meaning and empty list to populate,
and a `getContractTrace', that returns the hash of the trace. This pattern is
quite common in Python code that uses Object Oriented Programming (OOP), and is the
way that Python does it, since interfaces are not native to Python syntax.

Before we started developing the `BIRParametricTracer', we read the code of the legacy
tracer, to see how it was actually implemented, what were the main features and methods
needed, and eventually how to perform some low-level operations with numbers and
registers. Unfortunately, the legacy tracer was not fully implemented, and a lot
of methods and features were not implemented (they simply throw a `NotImplementedError').
So, for the majority of the code we wrote, we did our own research and done a
lot of trial and error. But this also gave us the freedom to choose the approach
we pcreferred the most, so we were able to structure the code in the way we felt
was the best, and eventually change our mind and re-factor it. In particular, we
modularized a lot more all the needed functions for low level operations, and
made the functions more robust. This was due to the advantages that modularizing
brought us in the Parser development, and in the Tracer as well.

The `BIRParametricTracer' works as follows:
\begin{itemize}
  \item For all the instruction that the CPU exetutes

  \item Checks and evaluates all the contract in the CPU. It means that all the
    contracts are parsed, and then the final result computed with the `evalExp()'
    function is stored in the trace

  \item Returns the computed trace
\end{itemize}

Note that we stress about the fact that we store only the last since there might
be some recursion, for example in a if then else, where the true statement is a
sum between the value of two registers. Therefore, to evaluate this, we have to do
first the sum and then return it, both as a result of the sum and the if then
else, but this value is in the trace just once. Or to eventually compute the
validity of the predicate, we need to perform some operations that yields a result,
that is not stored in the trace. To archive this, in the `evalExp()' function, we
added a parameter named `trace', that by default would be False, but if you passed
as True, it would store in the trace. So, on the main call you can put the value
to True, and then once the function performs recursive computation, the result
of intermediate steps is not be stored in the trace. In this function, there is
a big `match-case' statement, that maps all the keywords of the parsed `BEXP' class
into a specific function. Here is where the recognition of the expression that
was implemented in the tracer becomes necessary. This functions then yield the final
result of the computation, even performing recursion when needed. Since BIR allows
to have numeric values that are also represented with the precision (meaning 8
Bits, 16 Bits and so on), all the functions did not return the raw value of the computation,
but a tuple with the value and the precision, so that the caller could check that
precision were matching and also check for errors of overflow. Since most of the
operations that the tracer was able to evaluate were binary, a lot of the code to
perform that was not ready out of the box from the Python language, so we needed
to write some of the code from scratch. This took some time, but was worth it, since
now the code does not rely on external libraries.

In the end, the `BIRParametricTracer' implemented the following methods: \begin{verbatim}
    eval\_expr() -> Evaluate the expression and trace eventually
    eval\_pred() -> Evaluate the predicate
    eval\_unary_operations() -> Evaluate the unary operations 
    eval\_binary_operations() -> Evaluate the binary operations
    eval\_ifthenelse() -> Evaluate the if then else
    eval\_cast() -> Evaluate the cast
    eval\_den() -> Evaluate the variable in a register
    eval\_load() -> Evaluate the load from memory
    eval\_store() -> Evaluate the store in memory
    eval\_obs() -> Evaluate the operand
    signedToUnsigned() -> Convert a signed number to an unsigned one
    unsignedToSigned() -> Convert an unsigned number to a signed one
    cast() -> Cast a number to a different precision
    translate\_value() -> Given a value and a not-matching precision, translate it
\end{verbatim}

So, the final artifact of this step was adding the `BIRParametricTracer' to the file
`model.py' of the project and, as in the case for the parser, add the if statement
int the main that checks the configurations and eventually uses it as the default
tracer. This took some iterations and various meetings with the team of
researchers, to be sure that the behavior of the tracer was the expected one.
This means that some modules of the tracer was written several times, yielding in
the end a version that can fully and correctly trace the BIR contracts.

In next section, we cover how we were able to validate the tracing correctness, and
therefore move to the next step of the pipeline.

\section{Validating the tracing}
\label{cha:Validating the tracing} The tracing step of the pipeline is quite crucial,
since from it correctness depends all the following steps. So, the tests needed
to be quite robust and cover all the possibilities that could occur while using
the tracer. To archive this, we wrote two different sets of tests, of two different
kind. In this section we cover both of them. All of the tests, as in the case
for the parser has been done using the Python package `unittest', and its class `TestCase',
for code coherence, as in \cref{cha:Validating the parsing}. \\

The first family of tests was the one that checked that the output of the evaluation
function was actually correct, and therefore the computation made by the custom
function developed in the previous section was correct. This could be seen as the
unit test for this step of the pipeline. To archive this, for every possible construct
of the BIR language, we listed a number of possible inputs, and we hardcoded the
expected output. Then we executed the tracer and read the output it would
produce, and compare to the expected one. If they matched, the test was passed.
This was done for all the possible constructs, all the operation that the construct
allowed, with normal cases and limit cases. Most of the limit cases relied on possible
overflows or similar sort of binary-level issues. we wrote tests of nested
expression, to see if the recursion was handled correctly, and also tests with
data with different precisions, expecting errors.

We grouped the tests in sets, each one corresponding to a single construct, so that
we could create a unique test that did the whole testing for that construct in one
function. This allowed to make the testing pipeline more performant and to have
a more readable and extensible test suite, with way less useless and dispersive lines
of code. In the end, we wrote almost a hundred tests, that covered all the
possible cases that could occur while running the tracer, and this could easily be
extended in the future if there is the need or new expression are added, of for
even more robust and strict checking. \\

The second family of tests was the one that checked that the `BIRParametricTracer'
and the legacy tracer, if launched with equivalent contracts (even though
written in different languages) with the same underlying assembly instructions,
would yield the same trace, and therefore, assuming the correctness of the
legacy tracer, the correctness of the new one. This can be seen as the
integration test. This test was quite tricky since, as said at the beginning of
last section, the legacy tracer was not fully implemented, and some of the
constructs or operations were not there, so some tests were indeed impossible with
this setup. Despite that, the implemented section was enough to validate the most
used operations and constructs, allowing to have a big enough coverage of the
most frequent code snippets that occurs in CPU contracts, and leaving the rest to
the first family of tests. To do this tests, we also needed a basic model of the
CPU to run instructions on. So we created multiple assembly files, with various set
of instruction, to make the tracing more complex, adding branches, jumps and
modifications to register's values.

So, in this test family, we wrote multiple cases, in particular:
\begin{itemize}
  \item A basic test with just evaluation of an always true if then else,
    reading the program counter, in a linear assembly code

  \item A test with an always true it then else, reading another register than
    the program counter, in a linear assembly code

  \item A test with an always true if then else, reading the program counter, in
    a branched assembly code

  \item A test with an always true if then else, reading another register than
    the program counter, in a branched assembly code

  \item A test with constant values in a branched assembly code

  \item A test with non deterministic if then else outcome in a random-generated
    assembly code

  \item A test reading the program counter in a random-generated assembly code

  \item Multiple contracts in a random-generated assembly code
\end{itemize}

All this contracts were checks that we actively decided to implement in order to
validate correctness. A part from this list, some other contracts where given from
the researchers, since they were the contracts they were using to test the whole
existing pipeline of the project, that includes ours. Some of this contracts, were
in fact more complex and fully fledged cases of contracts, and not just atomic
tests such as this ones that we just listed. So it was worth seeing if, with
this bigger and complex suite of cases, the tracer would produce the same expected
and desired outcome.

After testing all this facets of the tracer, we were confident that the
implementation was correct, and that we could move on to the next step of the tool
chain, namely the generation of new contracts using Rosette.

\section{Generating the Rosette code}
\label{cha:Generating the Rosette code} The last step of the loop for this
pipeline was the generation of the constraint using the Rosette language. This meant
mainly writing a grammar in the Rosette syntax to fully describe the BIR language,
and then it would have generated the output on its own, leveraging its capabilities
and the existing part of the code that already did this for the legacy pipeline.

In order to archive this goal, we needed to first have an understanding of the
Rosette language and how it worked. Since it is a Racket-based language, it is a
functional meta-language, that is something we did not have much experience.
Also, the errors that it produces are not much informative, and the
documentation was not so helpful. There is also not a lot of support, since the
community is quite small, and made up especially of researchers and academics.

To give a grasp of what Rosette code actually looks like, we report a snippet of
code:

\begin{verbatim}
  (define-grammar (birexp)
    [bexp
    (choose
        (OPERAND-VALUE (?? integer?))
        (OPERAND-TYPE (?? integer?))
        (OPERAND-ACCESS (?? integer?))
        (BExp\_Const (?? Bit64))
        (BExp\_Den (BVar (REG (reg))))
        (BExp\_Cast (bcast) (bexp) (bimmt))
        (BExp\_UnaryExp (buop) (bexp))
        (BExp\_BinExp (bop) (bexp) (bexp))
        (BExp\_BinPred (bpred) (bexp) (bexp))
        (BExp\_IfThenElse (bexp) (bexp) (bexp))
        (BExp\_Load (BExp_Den (BVar "MEM" reg)) (bexp) BEnd_LittleEndian Bit64)
        (BExp\_Store (bexp) (bexp) (bendi) (bexp))
        (OPCODE (?? integer?))
    )]
  )
\end{verbatim}

This is the code that actually describes all the possible expression of the BIR language,
with all the expected types and values. As you can see, it is a quite complex
and verbose language. It is quite similar to list for all the parenthesis, and
it is easy to guess this is a meta-language.

So, we did a lot of trial and error, but we were able to make some progress, given
the help that Tiziano Marinaro, a PhD involved in the project gave us. He was one
of the developers with the most experience with the BIR language and Rosette itself.
His help has been crucial to understand the language and the syntax, and to be able
to write the grammar.

Once also this step was completed, and Rosette was able to generate back the
constraint of the contract in BIR, it was finally time to test the fully fledged
pipeline, as we see in the next section.

\section{Code migration and documentation}
\label{cha:Code migration and documentation} For all the time of development of
this thesis, we worked on a separate branch of the project, to allow other
developer and researchers continue the work on other vital features. This was also
a way to not break the existing code, and to allow for a more controlled and
safe development.

Once we completed the development of our artifacts, it was actually time to merge
the code into the main branch, to allow the researchers to start developing the
project with the BIR pipeline, to then be able to use the new features and the new
language.

The merge operation was quite smooth, since our contribution was quite isolated.
We mostly created new greenfield files to be added, and touched just one
specific file in the main code, that is the `model.py', where we added the
tracer. In that specific file, we just added some new code, without touching the
existing one, so there were no conflicts. The merge operation was done using `git',
since the project used this tool for versioning, as it is standard almost everywhere.
Done that, the contribution was then merged, the pipeline ready to be used and
the language switch completed.

Also, since not all the developers knew BIR right away, and some of them needed
to be able to the understand its syntax and semantic, there was the need of a brief
documentation, with all the BIR constructs, all the rules of the grammar, some
examples of the code and special cases.

To accomplish this, we wrote a file called `BIR-README.md', that contained all
the needed information detailed above. This file was then added to the main project.
It also contains a short and brief set of pointers to the paper that introduced
BIR language in the first place, to allow for a more in depth understanding of the
language. In the file, more specifically, there are a list of examples, at least
one for each construct, and the eventual special cases that could occur. To give
a concrete example, a special case that might arise is when you do not want to
trace anything, if a condition is false, but the BIR IF statement has both a
true and a false check. Therefore, the false check should be a special empty
constant, and not trace anything. To create this, the following syntax was used:
\begin{verbatim}
  (BExp\_IfThenElse (BExp_Const (bv 0 Bit1)) (BExp_Const (bv 1 Bit64)) (BExp_Const (bv \# Bit64)))
\end{verbatim}
The `\#' operator is used to flag the fact that the constant is not traced, and therefore
is empty.

With this brief and hands-on documentation, using BIR became easier for the
developers that have to use it.

Done this, the project had a new pipeline to then consume and produce CPU contracts
using BIR, as it was the goal of this thesis, and also had the needed documentation.
This was more or less the conclusion of the work done in this thesis, and the final
contribution to the project.

While we were working on this project, we also tried to research a way to
improve the performance and the robustness of the pipeline, and we thought about
switching to Rust. This parallel research project is discussed in the next and
last section of this chapter.

\chapter{Validating the pipeline}
\label{cha:Validating the pipeline} One of the most difficult task, also from a
conceptual point of view, was how to validate the whole pipeline. This is
because the huge number of variables that are involved makes the possible
outcomes and cases to test really large and almost impossible to cover entirely
in a test suite.

Therefore, we proposed the same approach that the legacy pipeline was using. It
consisted in running the tool using configuration files that specified that the
language used was, in fact, BIR and not the DSL anymore, to trigger the if statement
that we applied in \cref{cha:Parsing BIR} and \cref{cha:Tracing the parsed BIR}.
We then created a set of candidate contracts, following the ones used by the researchers
to test the legacy pipeline, and we run the tool on them to see the final output,
and test whether is met the expected one.

Here is a snippet of a configuration file that we used:
\begin{verbatim}
\# test-always-pc-bir.yaml
"contract\_observation_clause": "bir"
"contract\_target":
- "BExp\_IfThenElse (BExp_Const (bv 1 Bit1)) (BExp_Den (BVar (REG 7))) 
                (BExp\_Const (bv 11 Bit32))"
"executor": "contract\_executor"
"input\_gen_entropy_bits": !!int "9"
\end{verbatim}

This configuration file specifies that the language used is BIR, and that the
contract to be used is the one that traces the Program Counter, the register 7,
if a condition is true, and the value 11 otherwise. The executor is the contract
executor, that is the main class that runs the pipeline, and the input
generation entropy bits is set to 9, that is the number of bits that are used to
generate random values in the contract, to allow for more complex and random
contracts.

The output was not directly comparable, since one output would have been a set of
BIR constraints, while the other would have been a set of DSL constraints. Therefore,
the comparison was a way that could not be used. We then decided to have an expected
final contract and compare the output of the pipeline with the expected one.
This would have been the final validation that we needed to fully be sure that
the pipeline was doing the right thing and the final outcome was correct.

We implemented this test for a number of contracts from the one that the
researchers gave us for testing the tracer, that were part of the so-called testing
campaign of the project. Testing that meant to have the needed final validation
of the pipeline and the work of this thesis.

Here we propose some of the tests that we run, that are part of the testing campaign:
\begin{itemize}
  \item BExp\_IfThenElse (BExp\_Const (bv 1 Bit1)) (BExp\_Den (BVar REG 7)) (BE
    xp\_Const (bv 11 Bit32))
\end{itemize}

This first test is the simplest case. It evaluates always to true, and therefore
traces the value of the register 7, the Program Counter.

\begin{itemize}
  \item BExp\_IfThenElse (BExp\_BinExp BIExp\_And (BExp\_BinPred BIExp\_Equal (O
    PERAND-TYPE 0) (BExp\_Const (bv 1 Bit64))) (BExp\_BinPred BIExp\_Equal (OPERA
    ND-VALUE 0) (BExp\_Const (bv 0 Bit64))) (BExp\_Den (BVar REG 7)) (BExp\_Con st
    (bv \# Bit32))
\end{itemize}

The second test is a little bit more complex. It checks if the current operand
type is 1 and the operand value is 0. If this is true then it traces the Program
Counter again, otherwise it does not trace anything.

\begin{itemize}
  \item BExp\_IfThenElse (BExp\_BinExp BIExp\_And (BExp\_BinPred BIExp\_Equal (O
    PERAND-TYPE 0) (BExp\_Const (bv 1 Bit64))) (BExp\_BinPred BIExp\_Equal (OPERA
    ND-VALUE 0) (BExp\_Const (bv 0 Bit64))) (BExp\_Den (BVar REG 7)) (BExp\_Con st
    (bv \# Bit32))
\end{itemize}
The third test checks if the current OPCODE is 227 and if the operand value is 0,
and in that case it traces the Program Counter, otherwise it does not trace anything.

\begin{itemize}
  \item BExp\_IfThenElse (BExp\_BinExp BIExp\_And (BExp\_BinPred BIExp\_Equal (OPERAND-TYPE
    1) (BExp\_Const (bv 2 Bit64))) (BExp\_BinPred BIExp\_Equal (OPERAND-ACCESS 1)
    (BExp\_Const (bv 21 Bit64))) (OPERAND-VALUE 1) BExp\_Const (bv \# Bit32))
\end{itemize}

The last provided example of test checks if the OPERAND type is 1 and the
OPERAND access is 21, and if this is true, it traces the OPERAND value, otherwise
it does not trace anything. is 2 and if the operand access is 21, and if this is
true, it traces the operand value, otherwise it does not trace anything. The
provided examples are quite representative of the type of atomic tests that we
run to validate the pipeline.

This list is not comprehensive of all the constraints used for the testing, but gives
a good idea of all types, grammar and structure that we mostly used for this
purpose.

Other tests were more complex ones, usually with multiple constraints, such as
combining the ones provided above, and adding more of them. The most used
operations in the contracts were all the OPERAND related ones, since they are the
most meaningful triggers for speculative execution in the mind of the researchers.
This means that given the type of the operands and eventually their value or
their access, the cpu could make different speculations, so this is why they are
the most used in the test contracts.

Once we pass these testing contracts, we start to run the pipeline. As described
in the \cref{cha: Loop outlook}, we start with a clear contract, and we have the
passed ones as candidate contract. At this point, we start to run the pipeline, and
we check the final trace. With the final trace we can then infer what constraint
the CPU contract has, and we compare it with the expected one. We can then add
all of these needed constraints to the current contract and loop through the
pipeline again, up to the point where we reach the fixed point.

To then validate the pipeline, having the contract, knowing what contract we expect
once the simulation completed given the contract candidate exposed previously,
we expect a given final outcome, the contract that we expect to be generated. We
can therefore run the whole tool chain, involving the pipeline that we built in this
thesis, and then check if the final contract is the one that we expected. If that
is the case, then, the validation is passed and the pipeline is said to be correct.

We run the test provided above, plus a number of other tests, that are mostly a combination
of the provided ones, seeing the final outcome of the pipeline, and the provided
result.

An example of the output that the code produces for the first test is the following:
\begin{verbatim}
================================ Statistics ===================================
Test Cases: 2
Inputs per test case: 20.0
Required priming: 0
Flaky violations: 0
Violations: 0
Effectiveness: 
  Effectiveness: 0.0
  Total Cls: 13.3
  Effective Cls: 0.0
Filters:
  Speculation Filter: 0
  Observation Filter: 0

[-] No result tuple from fuzzer.
[+] Latest contract:

Current contract: {
        (BExp\_Den (BVar (REG 7)))
}
[+] Equivalence classes:
[5740354900026072187]
\end{verbatim}
As you can see, the existing contract is the one that we expected, and we have a
full statistic breakdown of the test run. The run takes 5.7 seconds with the base
settings.

After this process, the pipeline was validated by this test, but this tests also
showed us that the switch from the DSL to BIR had a cost in terms of performance.
This is because of the nature of the language. BIR is more complex and also tends
to be recursive in its evaluation, to allow more freedom and powerful expression.
This comes with a cost, that the Rosette constraints generation tends to be
quite slower than it previously was. Also the tracing step seems to take longer with
BIR, just due to its recursive nature, while the DSL was simpler and linear to evaluate.
Regardless, you can see the difference also in the output of the contract. While
in the DSL the output was always an IF statement, in BIR if the statement is always
true the system gets rid of the IF statement, making the constraint of the
contract shorter and more readable. This is a trade-off that was not foreseen,
and curious to see.

Doing the aftermath, in more deterministic terms, the BIR pipeline takes around
3 times the time that the DSL pipeline takes, and this is a cost that needs to be
paid for this more powerful pipeline and language. For the time being, the cost is
acceptable, and there seems not be a problem in the foreseeable future, but this
is something hard to assess at the moment.

In the end, regardless of the performance matter, this test gave us the final confirmation
that the pipeline was mature enough and ready to be merged into the main project,
to then let the other researchers continue the development of the other features,
finally using the desired BIR language. This transition and the brief
documentation produced is discussed in the next section.

\chapter{Migration to Rust}
\label{cha:Migration to Rust} During and after all the development described in the
previous steps, we had the idea of trying to write all the logic of the BIR's
pipeline in Rust. Empirically, Rust is a more performing language confronted to
Python, it offers a huge variety security features, and it is growing in
popularity. Therefore, switching from a pure Python implementation to a Rust one
(at least a Hybrid one) seemed a really logical choice. This also given the slight
performance hit we had in the pipeline final assessment session described in the
last chapter. We then started to make some research about how the Rust code
could be integrated and if ultimately, this was even a good idea. It turned out
that it was not, for this specific code. But let's see why. First, lets focus
deeply on the eventual pros that switching to Rust could have brought.

\section[Eventual pros]{Eventual pros}
As said before, Rust is way more performing than Python is. Naturally, for the majority
of the tasks, it runs on three orders of magnitude faster than Python. This due to
his lower-level nature. Rust is also a safer language, with numerous features that
prevent the developer from making mistakes. The borrow checker can be a pain
during development, but assures that the majority error never happen. The type
system of Rust is also one if its selling points. With it, it is able to create
more powerful structure that are a better implementation then they are in Python.
As described in \cref{cha:Parsing BIR}, the Parse tree class `BEXP' could have
been better handled with the power of Rust `enums' or using `trait' (the interfaces
of Rust) and generics . Rust is also a language that has the potential to be
more maintainable and extensible. This could be made by defining base traits (the
Rust interfaces, but more powerful) to, in the future, being able to insert new
parser if needed.

\section[Modularization]{Cons 1: Modularization}
It would have been a huge win if all the heavy lifting could be made by the Rust
code instead of the actual Python one. Looking at the codebase, it was clear
that that was not possible. This is because the codebase does not allow for modular
development. This is because a great amount the code relies on sharing some
global variables, that all the steps share. Also, there are external libraries called
at each step, so that code was not easily movable to Rust. In the end, it made
no sense to modularize the entire pipeline in one unique Rust codebase, since it
was called at each step of the pipeline in an independent way. This meant that, in
order to switch, we had to create numerous different Rust microservices, and call
each one of them via bindings, and then also re-elaborate the output, to be compatible
with the Python code. This would have been a huge amount of work, and it would have
been a source of bugs and technological debt. This, turns out in one of following
sections, was the ultimate reason also for not having any substantial
performance gain.

An alternative could have been switching and rewriting the whole codebase in
Rust. This could have been even more painful, due to the lack of some libraries,
as we see in next section.

\section[Libraries support]{Cons 2: Libraries support}
As the time of writing, there is no robust and stable library for parsing
written in Rust. This meant that we needed to write a parser library from scratch.
This has been done, via a small proof of concept. The library is called `Pilator',
and its only capability, at the moment, is allowing users to define their own Regex-like
language, and parse the strings with it. It is effectively a PEG as `pyparsing',
but way less powerful. It is done with a simple backtracking algorithm, and it
is not fully optimized. It was just to see if there was any potential in this path.
The library is available on GitHub, \cite{pilator}. Writing this library has
highlighted the fact that writing this is a whole project in itself, with a too much
complexity, bugs, edge cases and optimizations to be made. This was only a
source of technological debt for the codebase. If a bug was to be discovered and
year from the time of writing, there was anyone maintaining and fixing the
library, and the whole project would be stuck.

A lot of other libraries that are used in the codebase do not have a counter part
in the Rust ecosystem, so this would mean even more writing from scratch, that would
take too much precious time, not worth the effort.

\section[Real performance]{Cons 3: Real performance}
The second problem with this implementation is that the main code is still
written in Python, therefore we needed to modularize each component of the pipeline
in itself. We could not switch it entirely and then call directly the Rust code,
as previously described. This brings the bottleneck of the bindings needed to
communicate between the Python and the Rust code. To check if, performance-wise,
this could make sense anyway, we set up an ad-hoc test suite. By writing with bindings,
simply a function sums two numbers in Rust, using the `Maturing' `Py03' Library,
the most known and developed library for this purpose. To do just so, it takes
around 10 seconds for each compile, and 11 nanoseconds to sum two numbers.
Python to do a simple sum takes around 30 nanoseconds. we also decided to test the
performance of the existing Python code for parsing, and compare with also the
Rust one. It turns out that Python takes and average of 3 milliseconds to parse a
single line of BIR language. Pure Rust takes an average of 0.002 milliseconds, while
Rust with bindings takes on average 0.07.

Given these data, it could seem that using bindings could be a huge performance
gain over the pure Python implementation, but there are some caveats. First, we need
to note that this is just for the parsing part of the pipeline, and the other
parts could have different performance. Also, the parsing part is not the
bottleneck of the pipeline, given the really short time that also the Python code
takes. Note that the average length of a BIR string is around 45 characters, so the
parsing time is really short. We need to also keep into consideration that these
performance gain, that is not even noticeable, since parsing is not the most
intensive part of the pipeline, comes at the cost of a more complex codebase,
with the need to maintain a new source code and language, that is less mature than
the solution already in place with Python and `pyparsing'. The most intensive
step of the pipeline is the generation of the constraints, that is done with Rosette,
so does not entail switching from Python to Rust. This was another part of the stack
that was not really negotiable, since the expertise was already in place in the
researchers group and the powerful tooling that Rosette offers was not available
in Rust.

In the end, for a negligible and not even noticeable performance gain at inference
time, we have a huge cost at development time, more complex and distributed code,
and technical debt that could be a source of bugs and issues in the future. Given
that, also performance-wise, the gains this switch could bring are not worth the
effort. All the benchmark code can be found on GitHub, \cite{benchmark}.

\section[Real extensibility]{Cons 4: Real extensibility}
Another point that we made about switching to Rust was the eventual extensibility
of the produced code, allowing for future new parser and eventual other
components. If we created a `parser` trait, then any eventual additional
representation language could have been the same trait, and therefore, once
tested there was no need in the actual code to handle the new addition. This is a
really useful feature, if you ever use it.

As we thought, the scope of this thesis was creating a new pipeline to handle the
BIR language, that was more robust and researched for representing a ISA of a
real CPU. How many other times there is the need to implement a new language for
the ISA? Probably never, since we choose BIR for his empirical superiority.
There are not even much other candidate to do so, and BIR seems to be, as today,
the best. Therefore, the extensibility of the code was not a real concern, and
it was not worth the effort to switch to Rust for this reason.

\section[Development speed]{Cons 5: Development speed}
Another fact is that, compared to Python, Rust is way slower to develop. This is
because of the borrow checker, that is a really powerful tool, but it can be a pain
to work with. In this case, development performance is more important, since
this is a research project. Also the learning curve of the language is way steeper
than Python, and the community is way smaller. So, being able to do things and
destroy afterwards, without spending astonishing amount of times was a huge win,
and for this Python was just the best. Plus, on the team there was no high
expertise in Rust. This would have meant technical debt to be paid in the future,
and huge amount of time spent on learning the language. Given the type of project
in itself, this was not worth the effort.

\section[Safety]{Cons 6: Safety}
One last point is on safety. This could be a deal-breaker for some cases, where
the code is really critical. But in this case, the code is not critical, and it is
used to synthesize contracts from the CPU, from a local computer. This means no huge
throughput, not real race conditions, and probably not real issues at all with memory
management.

We are not dealing with data of person or any sensible information, so the
safety for this project was not a real concern.

Ultimately, the safety that Rust would have naturally given to the code, was not
something that was strictly required, nor even much useful.

\section[Conclusion]{Conclusion}
After all the research that we have done, that has been described in precedent
section, it was clear that switching to Rust was not worth the effort.

In the end, we focused our attention in writing the best Python code, and leveraging
the advantages that the language offers. Despite the frankly unexpected outcome,
the research made to understand that Rust was not a great fit let to a lot of learning
both in term of how both the languages works, their pros and cons, and how to evaluate
which one is the best for a specific project. Also, in order to write the PoC of
the parser and the benchmarks, we had the chance both to write Rust code, and learn
more about it, and also to see how it is to actually write a really complicated
piece of software as a parser from scratch, and this was a really valuable experience.

This also showed us how it actually works the world of research, where the
majority of the time is spent in thinking how to do and if it makes sense to do
so, in spite of the actual implementation.
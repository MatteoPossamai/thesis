\chapter{Core}
\label{cha:core}

\section{Parsing BIR}
\label{cha:Parsing BIR} The first step in the pipeline is parsing the BIR
language. We start from a raw string given as input, and we first want to
detemine whether this string is valid BIR code, and if it is the case, parse it
into some readable and consumable data structure that we can use in following
steps. In order to archieve this goal, we first needed to have in mind a clear
idea of the semantic of the language, the grammar and the regex that it is generated
from. In order to do so, I read with the paper that defines the language (\cite{bir_pub}),
and had various chats with the researchers who are in charge of the project, with
specific focus on the BIR language. With them, I understood the grammar, the
main features and constructs of the language, and I began to be familiar with the
semantic. I wrote some examples of the code in the section \ref{cha:BIR}. We decided
so, that we needed all the basics constructs of the language, so the if then else,
the binary operations, the memory operations, the unary operations, the predicates,
constant values and variables. After some time, we also decided to add the
OPCODE operators, since they became needed for the project as a whole.

Once I had this clear idea in mind, it was time to start developing the new
parser. I did it following the same approach used to developed the previous one,
but adaping the code to the specifics of BIR. I wrote the parser using the Python
library `pyparsing' (\cite{pyparsing}). It is a Parsing Expression Grammar (PEG)
parser, meaning that it is able to parse given strings starting from a regular expression
(or more than one) generated by the developer. This library is really powerful, and
it is able to parse complex languages, with a lot of features. It is also quite easy
to use, and with good documentation. It has different built-in functions that allows
to create powerful regex, such as the option for creating object that appears `OneOfMore'
times, or `ZeroOrMore' times, and so on. This allows the developer to focus more
on the regex of the language itself, without the need to focus on complex
details about the implementation of the parsing.

Then, to develope this parser, I dived deep into the language and created all
the regular expressions needed to represent the full specification of the
language itself. Since it can generate quite long string, a lot of the code was
split in different subfunctions, objects and routines in the file, to allow
better code reuse and less confusion fot future readers and developers.

Once the grammar was fully created, I created the class structure that `pyparsing'
needs to put all the parsed string, so we can after the parsing also trace and
evaluate the expression. I created this class, that according to the generated
keyword, it would have some specific attributes that will be used in future steps
of the pipeline. This class is called `BEXP', as Binary Expression. This class
is istantiated when the parser finds a binary expression, and it is used to the eventual
operands, or operation types. Also this section of the code has been modularized
as much as possible, so in case there is the need to add new constructs or
features it should be as easy as possible.

I did this the first time with all the specification created in the first
meetings, and then added when the necessity arised, also the par for the parsing
of the OPCODE operators. In the end, the artifact for this step was a Python
file with multiple classes and functions, that allows the parsing of all valid BIR
strings, and the creation of a `BEXP' object accoring to the parsing string
content. In case this is not possible, do to some errors, it throws an exception
error, with the character that caused the error, since not expected. `pyparsing'
demostred to be extremely powerful, very user friendly, and fast both in terms
of runtime execution, but also in writing time needed, debugging and learing
curve. And since it was already used from the other members of the team, it does
not introduce complexity or any technical debt of any kind, so in future can be maintained
and updated with ease.

\section{Validating the parsing}
\label{cha:Validating the parsing} The second step in the pipeline is validating
the parsing. Since this is the first and most critical step in the pipeline, it needed
a quite big and comprehensive test suite, to certify its correctness. In order to
archieve this, for each possible construct of the language, I wrote different test
cases. They could be tests where a valid string was parsed, and I checked that the
class instance output contained in fact what I wanted and expected from it.
Another type of test that I implemented was the creation of invalid BIR string, and
I expected a failure from the parser. If that would not happen, then there was some
issue in the current implementation.

For all the testing, the Python library `unittest' was used. This library allows
to create a `TestCase' class, in which developers can define their tests. THe class
allows for a `setUp' method that is executed before the whole testcase is. Then,
all the methods that has a function name starting with `test' are executed. In
the function, I inserted many assert statements, to check that the outcome of
the executed function was what I expected. If the test runs as expected, all the
assertions are met, the the test is counted as passed, otherwise it fails. In the
end of the simulation, the library also provides a brief summary of all the
executed tests, how many passed and the error that caused the failure of the failed
ones. Also this library has proven to be extremely powerful and capable. Making it
easy to write the test suite. In fact, this library has been used also in all the
testing done in this thesis, in the steps \ref{cha:Validating the tracing} and
\ref{cha:Validating the tracing}. To then, in the end be able to run all the
generate tests, I also wrote a brief shell script, which executed all the tests
and made some prettier printing of the results.

Doing this part has been extremely useful, since it allowed to check that the current
implementation was correct, and also in the meantime, while correcting the errors,
be sure that I did not broke any working piece of the software. The last part of
the implementation has been done alongside the testing, since this improved the
speed and productivity of code writing and effective artifacts produced. This allows
in the future, to do some addition to the parser, and test automatically in a matter
of milliseconds if the implemented change did not break anything.

\section{Tracing the parsed BIR}
\label{cha:Tracing the parsed BIR} The third step in the pipeline is tracing the
parsed BIR.

\section{Validating the tracing}
\label{cha:Validating the tracing} The fourth step in the pipeline is validating
the tracing.

\section{Generating the Rosette code}
\label{cha:Generating the Rosette code} The fifth step in the pipeline is generating
the Rosette code.

\section{Validating the pipeline}
\label{cha:Validating the pipeline} The sixth step in the pipeline is validating
the pipeline.

\section{Migration to Rust}
\label{cha:Migration to Rust} During and after all the development described in the
previous steps, there was the idea of trying to write all the logic of the
toolchain in Rust. Empirically, Rust is a more performant language confronted to
Python, it offers a hige variety security features, and it is increasing in
popularity. Therefore, switching from a pure Python implementation to a Rust one
seemed the most logical choice. I started to make some research about how the Rust
code could be integrated and if ultimately, this was even a good idea. It turned
out that it was not, for this specific codebase. But let's see why. First, lets focus
deeply on the eventual pros that switching to Rust could have brought.

\subsection[Eventual pros]{Eventual pros}
As said before, Rust is way more performant than Python. Naturally, for the
majority of the tasks, it runs on three orders of magnitude faster than Python.
This due to his lower-level nature. Rust is also a safer language, with numerous
features that prevent the developer from making mistakes. The borrow checker can
be a pain during development, but assures that the majority error never happen. Rust
is also a language that has the potential to be more maintainable and extensible.
This could be made by defining base traits (the Rust interfaces, but more
powerful) to, in the future, being able to insert new parser if needed.

\subsection[Modularization]{Cons 1: Modularization}
It would have been a huge win if all the heavy lifting could be made by the Rust
code instead of the actual Python one. Looking at the codebase, it was clear that
that was not possible. This is because the codebase does not allow for modular
development. This is because a great amount the code relies on sharing some global
variables, that all the steps share. Also, there are external libraries called
at each step, so that code was not upgradable to Rust. In the end, it made no sense
to modularize the entire pipeline in one unique Rust codebase, since it was
called at each step of the pipeline in an independent way. This meant that, in
order to switch, we had to create numerous different Rust microservices, and
call each one of them via bindings, and then also re-elaboreate the output, to
be compatible with the Python code. This would have been a huge amount of work,
and it would have been a source of bugs and technological debt. This, will turn
out in one of following subsections, was the ultimate reson also for not having any
substantial performance gain.

\subsection[Libraries support]{Cons 2: Libraries support}
As the time of writing, there is no robust and stable library for parsing
written in Rust. This meant that I needed to write a parser library from scratch.
This has been done, via a small proof of concept. The library is called Pilator,
and its only capability, at the moment, is allowing users to define their own Regex-like
language, and parse the strings with it. It is done with a simple backtracking algorithm,
and it is not optimized. It was just to see if there was any potential in this path.
The library is available on GitHub (\cite{pilator}). Wrinting this library has highlighted
the fact that writing this is a whole project in itself, with a too much
complexity, bugs, edge cases and optimizations to be made. This was only a source
of technological debt for the codebase. If a bug was to be discovered and year
from the time of writing, there was anyone maintaining and fixing the library,
and the whole project would be stuck.

\subsection[Real performance]{Cons 3: Real performance}
The second problem with this implementation is that the main code is still
written in Python, therefore we needed to modularize each component of the pipeline
in itself. We could not switch it entirely and then call directly the Rust code.
This brings the bottleneck of the bindings needed to communicate between the Python
and the Rust code. To check if, performace-wise, this could make sense anyway, I
set up a small test. By writing with bindings, simply a function sums two numbers
in Rust, using the Maturing Py03 Library, the most known and developed library for
this purpose. To do just so, it takes around 10 seconds for each compile, and 11
nanoseconds to sum two numbers. Python to do a simple sum takes around 30 nanoseconds.
I also decided to test the performance of the existing Python code for parsing. It
turns out that it takes and average of 3 milliseconds to parse a single line of BIR
language. This means that, in a system where it will be rare that there will be more
than 1000 lines of BIR code, the parser does not really represent a bottleneck, given
the fact that we are using Rosette to generate new code, and continuously
calling the solver. According to this number, and the fact that the library we
would be using would not be as optimized as the Python one (`pyparsing`), the performance
gain would be minimal, if not negative. And all would cost huge amount of time for
developing and debugging a new solution from scratch.

All the benchmark code can be found on GitHub (\cite{benchmark}).

\subsection[Real extensibility]{Cons 4: Real extensibility}
Another point that I made about switching to Rust was the eventual extensibility
of the produced code, allowing for future new parser and eventual. If we created
a `parser` trait, then any eventual additional language could have been the same
trait, and therefore, once tested there was no need in the actual code to handle
the new addition. This is a really useful feature, if you ever use it.

As we thought, the scope of this thesis was creating a new pipeline to handle the
BIR language, that was more robust and researched for representing a ISA of a
real CPU. How many other times there will be the need to implement a new language
for the ISA? Probably never. There are not even much other candidate to do so,
and BIR seems to be, as today, the best. Therefore, the extensibility of the
codebase was not a real concern, and it was not worth the effort to switch to Rust
for this reason.

\subsection[Development speed]{Cons 5: Development speed}
Another fact is that, compared to Python, Rust is way slower to develop. This is
because of the borrow checker, that is a really powerful tool, but it can be a
pain to work with. In this case, development performance is more important, since
this is a research project. So, beigng able to do things and destroy afterwards,
without spending astonishing amount of times was a huge win, and for this Python
was just the best. Plus, on the team there was no high expertise in Rust. This
would have meant technical debt to be paid in the future, and hufe amout of time
spent on learning the language. Given the type of project in itself, this was not
worth the effort.

\subsection[Safety]{Cons 6: Safety}
One last point is on safety. This could be a deal-breaker for some cases, where the
codebase is really critical. But in this case, the codebase is not critical, and
it is used to synthesize contracts from the CPU, from a local computer. This
means no huge throughput, not real race conditions, and probably not real issues
at all with memory management.

Ultimately, the safety that Rust would have naturally given to the codebase, was
not something that was stricly required, nor even much useful.
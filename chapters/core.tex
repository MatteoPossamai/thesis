\chapter{Core}
\label{cha:core}

\section{Parsing BIR}
\label{cha:Parsing BIR} The first step in the pipeline is parsing the BIR
language. We start from a raw string given as input, and we first want to
detemine whether this string is valid BIR code, and if it is the case, parse it
into some readable and consumable data structure that we can use in following
steps. In order to archieve this goal, we first needed to have in mind a clear
idea of the semantic of the language, the grammar and the regex that it is generated
from. In order to do so, I read with the paper that defines the language (\cite{bir_pub}),
and had various chats with the researchers who are in charge of the project, with
specific focus on the BIR language. With them, I understood the grammar, the
main features and constructs of the language, and I began to be familiar with the
semantic. I wrote some examples of the code in the section \ref{cha:BIR}. We decided
so, that we needed all the basics constructs of the language, so the if then else,
the binary operations, the memory operations, the unary operations, the predicates,
constant values and variables. After some time, we also decided to add the
OPCODE operators, since they became needed for the project as a whole.

Once I had this clear idea in mind, it was time to start developing the new
parser. I did it following the same approach used to developed the previous one,
but adaping the code to the specifics of BIR. I wrote the parser using the Python
library `pyparsing' (\cite{pyparsing}). It is a Parsing Expression Grammar (PEG)
parser, meaning that it is able to parse given strings starting from a regular expression
(or more than one) generated by the developer. This library is really powerful, and
it is able to parse complex languages, with a lot of features. It is also quite easy
to use, and with good documentation. It has different built-in functions that allows
to create powerful regex, such as the option for creating object that appears `OneOfMore'
times, or `ZeroOrMore' times, and so on. This allows the developer to focus more
on the regex of the language itself, without the need to focus on complex
details about the implementation of the parsing.

Then, to develope this parser, I dived deep into the language and created all
the regular expressions needed to represent the full specification of the
language itself. Since it can generate quite long string, a lot of the code was
split in different subfunctions, objects and routines in the file, to allow
better code reuse and less confusion fot future readers and developers.

Once the grammar was fully created, I created the class structure that `pyparsing'
needs to put all the parsed string, so we can after the parsing also trace and
evaluate the expression. I created this class, that according to the generated
keyword, it would have some specific attributes that will be used in future steps
of the pipeline. It behaves as a generic type, or a sort of enum, according to
the specific construct it is modeling. This approach was not quite optimal, you
cannot infer directly what type of expression is being represented, unless you check
the used attributes. So, I added the `keyword' attribute, to make clear and
easier to understand what type of expression was used and allow for more
readable code. Other languages different from Python, such as Rust, can do this with
generics or enums, that are features not offered by Python. This is one of the reasons
that made us thinking about switching the language of this thesis's project to Rust,
as we will see in the section \ref{cha:Migration to Rust}.

This class is called `BEXP', as Binary Expression. This class is istantiated
when the parser finds a binary expression, and it is used to the eventual operands,
or operation types. Also this section of the code has been modularized as much as
possible, so in case there is the need to add new constructs or features it
should be as easy as possible.

I did this the first time with all the specification created in the first
meetings, and then added when the necessity arised, also the par for the parsing
of the OPCODE operators. In the end, the artifact for this step was a Python
file with multiple classes and functions, that allows the parsing of all valid BIR
strings, and the creation of a `BEXP' object accoring to the parsing string
content. In case this is not possible, do to some errors, it throws an exception
error, with the character that caused the error, since not expected. `pyparsing'
demostred to be extremely powerful, very user friendly, and fast both in terms
of runtime execution, but also in writing time needed, debugging and learing
curve. And since it was already used from the other members of the team, it does
not introduce complexity or any technical debt of any kind, so in future can be maintained
and updated with ease.

To introduce the class in the codebase, it was enough to add a if statement, that
accoring to the configuration used to run the project, if the selected language
was `bir', then the used parser would have been `BIRParser'.

\section{Validating the parsing}
\label{cha:Validating the parsing} Then it was time for validating the parsing.
Since this is the first and most critical step in the pipeline, it needed a quite
big and comprehensive test suite, to certify its correctness. In order to archieve
this, for each possible construct of the language, I wrote different test cases.
They could be tests where a valid string was parsed, and I checked that the class
instance output contained in fact what I wanted and expected from it. Another
type of test that I implemented was the creation of invalid BIR string, and I expected
a failure from the parser. If that would not happen, then there was some issue in
the current implementation.

For all the testing, the Python library `unittest' was used. This library allows
to create a `TestCase' class, in which developers can define their tests. THe class
allows for a `setUp' method that is executed before the whole testcase is. Then,
all the methods that has a function name starting with `test' are executed. In
the function, I inserted many assert statements, to check that the outcome of
the executed function was what I expected. If the test runs as expected, all the
assertions are met, the the test is counted as passed, otherwise it fails. In the
end of the simulation, the library also provides a brief summary of all the
executed tests, how many passed and the error that caused the failure of the failed
ones. Also this library has proven to be extremely powerful and capable. Making it
easy to write the test suite. In fact, this library has been used also in all the
testing done in this thesis, in the steps \ref{cha:Validating the tracing} and
\ref{cha:Validating the tracing}. To then, in the end be able to run all the
generate tests, I also wrote a brief shell script, which executed all the tests
and made some prettier printing of the results.

Doing this part has been extremely useful, since it allowed to check that the current
implementation was correct, and also in the meantime, while correcting the errors,
be sure that I did not broke any working piece of the software. The last part of
the implementation has been done alongside the testing, since this improved the
speed and productivity of code writing and effective artifacts produced. This allows
in the future, to do some addition to the parser, and test automatically in a matter
of milliseconds if the implemented change did not break anything.

\section{Tracing the parsed BIR}
\label{cha:Tracing the parsed BIR} The next step in the pipeline was tracing the
parsed BIR code. This means that I needed to create a tracer, a piece of software
that, given the contract written in BIR and the state of the CPU, called `model'
in this project, it would be able to populate the trace. The trace is a list data
structure that contains all the information that passes through the CPU. So, if I
ask with a BIR contract to evaluate a conditional statement (if then else) with a
true condition, and then accoringly return the value in the register RAX or RBX,
I expect to find in the trace the value contained in the RAX register.

The step of tracing is really important, because this allow us to check if the
contract leaks more information than what we expect. With this, we are then able
to add more contracts to the CPU, and then check loop over again, until we reach
a fixed point, as detailed in the section \ref{cha: Loop outlook}.

To accomplish this goal of tracing the contracts, populating the trace and
allowing the project to run his loop, I needed to create a new class for this purpose.
To maintain the codebase as close as his original state, and not bring too much complexity,
I followed the same approach used for the legacy tracer for the DSL. I created the
class `BIRParametricTracer', that is a subclass of `UnicornTracer'. The `UnicornTracer'
class is a defined class in the codebase of the Malcos repository, and it is a
subclass of `ABC' class, that is a Python standard abstract class. The `UnicornTracer'
class is used as an interface inside the codebase, to allow for the creation of different
tracers, and all will have a trace, meaning and empty list to populate, and a `getContractTrace',
that returns the hash of the trace.

Before I started developing the `BIRParametricTracer', I read the code of the
legacy tracer, to see how it was actually implemented, what were the main
features and methods needed, and eventually how to perform some low-level operations
with numbers and registers. Unfortunately, the legacy tracer was not fully implemented,
and a lot of methods and features were not implemented. So, for the majority of
the code I wrote, I did my own research and done a lot of trial and error. But
this also gave me the freedom to choose the approach I prefered the most, so I was
able to structure the code in the way I felt was the best. In particular, I modularized
a lot more all the needed functions for low level operations, and made the
functions more robust.

The `BIRParametricTracer' works as follows: For all the instruction that the CPU
executes, it then checks and runs all the contract in the CPU. It means that all
the contracts are parsed, and then the final result computed with the `evalExp'
function is stored in the trace. I stress about only the last since there might be
some recursion, for example in a if then else, where the true statement is a sum
between the value of two registers. Therefore, to evaluate this, I have to do
first the sum and then return it, both as a result of the sum and the if then else,
but this value will be in the trace just once. To archieve this, in the `evalExp'
function, I added a parameter named `trace', that by default would be False, but
if you passed as True, it would store in the trace. So, on the main call you can
put the value to True, and then once the function performs recursive computation,
the result of intermediate steps will not be stored in the trace. In this function,
there is a bit `match-case' statement, that maps all the keywords of the parsed `BEXP'
class into a specific function. This functions then yield the final result of
the computation, even performing recursion when needed. Since BIR allows to have
numeric values that are also represented with the precision (meaning 8 Bits, 16
Bits and so on), all the functions did not return the raw value of the
computation, but a tuple with the value and the precision, so that the caller
could check that precision were matching and also check for errors of overflow. Since
most of the operations tha the tracer was able to evaluate were binary, a lot of
the code to perform that was not ready out of the box from the Python language, so
I needed to write some of the code from scratch. This took some time, but was worth
it, since now the code does not rely on external libraries.

In the end, the `BIRParametricTracer' implemented the following methods:
\begin{verbatim}
    eval_expr() -> Evaluate the expression and trace eventually
    eval_pred() -> Evaluate the predicate
    eval_unary_operations() -> Evaluate the unary operations 
    eval_binary_operations() -> Evaluate the binary operations
    eval_ifthenelse() -> Evaluate the if then else
    eval_cast() -> Evaluate the cast
    eval_den() -> Evaluate the variable in a register
    eval_load() -> Evaluate the load from memory
    eval_store() -> Evaluate the store in memory
    signedToUnsigned() -> Convert a signed number to an unsigned one
    unsignedToSigned() -> Convert an unsigned number to a signed one
    cast() -> Cast a number to a different precision
    translate_value() -> Given a value and a not-matching precision, translate it
\end{verbatim}

So, the final artifact of this step was adding the `BIRParametricTracer' to the
file `model.py' of the project and, as in the case for the parser, add the if
statement that checks the configurations and eventually uses it as the default
tracer. This took some iteration and various meetings with the team of researchers,
to be sure that the behauvior of the tracer was the expected one. This means that
some modules of the tracer was written several times, yielding in the end a
version that can fully and correctly trace the BIR contracts.

In the next section, we will cover how we were able to validate the tracing correctness,
and therefore move to the next step of the pipeline.

\section{Validating the tracing}
\label{cha:Validating the tracing} The tracing step of the pipeline is quite
crucial, since from it correctness depends all the following steps. So, the tests
needed to be quite robust and cover all the possibilities that could occur while
using the tracer. To archieve this, I wrote two different sets of tests, of two
different kind. In this section I will cover both of them. All of the tests, as in
the case for the parser has beed done using the Python package `unittest', and
its class `TestCase', for code coherence. \\

The first family of tests was the one that checked that the output of the evaluation
function was actually correct, and therefore the computation made by the custom
function developed in the previous section was correct. To archieve this, for
every possible construct of the BIR language, I listed a number of possible inputs,
and I hardcoded the expected output. Then I executed the tracer and read the output
it would produce, and compare to the expected one. If they matched, the test was
passed. This was done for almost all the possible constructs, all the operation
that the construct allowed, with normal cases and limit cases. Most of the limit
cases relied on possible overflows or similar sort of binary-level issues. I wrote
tests of nested expression, to see if the recursion was handled correctly, and also
tests with data with different precisions, expecting errors.

I grouped the tests in sets, each one corresponding to a single construct, so
that I could create a unique test that did the whole testing for that construct
in one function. This allowed to make the testing pipeline more performant and to
have a more readable and extensible test suite. In the end, I wrote almost a hundred
tests, that covered all the possible cases that could occur while running the
tracer, and this could easily be extended in the future if there is the need or new
expression are added. \\

The second family of tests was the one that checked that the `BIRParametricTracer'
and the legacy tracer, if runned with equivalent contracts (even though written in
different languages), would yield the same trace, and therefore, assuming the correctness
of the legacy tracer, the correctness of the new one. This test was quite tricky
since, as said at the beginning of last section, the legacy tracer was not fully
implemented, and some of the constructs or operations were not there, so some tests
were indeed impossible with this setup. Despite that, the implemented section was
enough to validate the most used operations and constructs, allowing to have a
big enough coverage of the most frequent code snippets that occurs in CPU
contracts, and leaving the rest to the first family of tests. To do this tests, I
also needed a basic model with instruction. So I created multiple assembly files,
with various set of instruction, to make the tracing more complex, adding branches,
jumps and modifications to register's values.

So, in this test family, I wrote multiple cases, in particular:
\begin{itemize}
  \item A basic test with just evaluation of an always true if then else,
    reading the program counter, in a linear assembly code

  \item A test with an always true it then else, reading another register than
    the program counter, in a linear assembly code

  \item A test with an always true if then else, reading the program counter, in
    a branched assembly code

  \item A test with an always true if then else, reading another register than
    the program counter, in a branched assembly code

  \item A test with constant values in a branched assembly code

  \item A test with non deterministec if then else outcode in a randomically-generated
    assembly code

  \item A test reading the program counter in a randomically-generated assembly code

  \item Multiple contracts in a randomically-generated assembly code
\end{itemize}

A part from this list, some other contracts where given from the researchers, since
they were the contracts they were using to test the whole existing pipeline of the
project, that will include mine. Some of this contracts, were in fact real-CPU
contracts, and not just atomic tests such as this ones that I just listed. So it
was worth seeing if, with this bigger and complex suite of cases, the tracer
would produce the same expected and desired outcome.

After testing all this facets of the tracer, I was confident that the implementation
was correct, and that I could move on to the next step of the toolchain, namely the
generation of new contracts using Rosette.

\section{Generating the Rosette code}
\label{cha:Generating the Rosette code} TODO: write rosette section

\section{Validating the pipeline}
\label{cha:Validating the pipeline} TODO: write validation section

\section{Migration to Rust}
\label{cha:Migration to Rust} During and after all the development described in the
previous steps, there was the idea of trying to write all the logic of the
toolchain in Rust. Empirically, Rust is a more performant language confronted to
Python, it offers a hige variety security features, and it is increasing in
popularity. Therefore, switching from a pure Python implementation to a Rust one
seemed the most logical choice. I started to make some research about how the Rust
code could be integrated and if ultimately, this was even a good idea. It turned
out that it was not, for this specific codebase. But let's see why. First, lets focus
deeply on the eventual pros that switching to Rust could have brought.

\subsection[Eventual pros]{Eventual pros}
As said before, Rust is way more performant than Python. Naturally, for the
majority of the tasks, it runs on three orders of magnitude faster than Python.
This due to his lower-level nature. Rust is also a safer language, with numerous
features that prevent the developer from making mistakes. The borrow checker can
be a pain during development, but assures that the majority error never happen. Rust
is also a language that has the potential to be more maintainable and extensible.
This could be made by defining base traits (the Rust interfaces, but more
powerful) to, in the future, being able to insert new parser if needed.

\subsection[Modularization]{Cons 1: Modularization}
It would have been a huge win if all the heavy lifting could be made by the Rust
code instead of the actual Python one. Looking at the codebase, it was clear that
that was not possible. This is because the codebase does not allow for modular
development. This is because a great amount the code relies on sharing some global
variables, that all the steps share. Also, there are external libraries called
at each step, so that code was not upgradable to Rust. In the end, it made no sense
to modularize the entire pipeline in one unique Rust codebase, since it was
called at each step of the pipeline in an independent way. This meant that, in
order to switch, we had to create numerous different Rust microservices, and
call each one of them via bindings, and then also re-elaboreate the output, to
be compatible with the Python code. This would have been a huge amount of work,
and it would have been a source of bugs and technological debt. This, will turn
out in one of following subsections, was the ultimate reson also for not having any
substantial performance gain.

\subsection[Libraries support]{Cons 2: Libraries support}
As the time of writing, there is no robust and stable library for parsing
written in Rust. This meant that I needed to write a parser library from scratch.
This has been done, via a small proof of concept. The library is called Pilator,
and its only capability, at the moment, is allowing users to define their own Regex-like
language, and parse the strings with it. It is done with a simple backtracking algorithm,
and it is not optimized. It was just to see if there was any potential in this path.
The library is available on GitHub (\cite{pilator}). Wrinting this library has highlighted
the fact that writing this is a whole project in itself, with a too much
complexity, bugs, edge cases and optimizations to be made. This was only a source
of technological debt for the codebase. If a bug was to be discovered and year
from the time of writing, there was anyone maintaining and fixing the library,
and the whole project would be stuck.

\subsection[Real performance]{Cons 3: Real performance}
The second problem with this implementation is that the main code is still
written in Python, therefore we needed to modularize each component of the pipeline
in itself. We could not switch it entirely and then call directly the Rust code.
This brings the bottleneck of the bindings needed to communicate between the Python
and the Rust code. To check if, performace-wise, this could make sense anyway, I
set up a small test. By writing with bindings, simply a function sums two numbers
in Rust, using the Maturing Py03 Library, the most known and developed library for
this purpose. To do just so, it takes around 10 seconds for each compile, and 11
nanoseconds to sum two numbers. Python to do a simple sum takes around 30 nanoseconds.
I also decided to test the performance of the existing Python code for parsing. It
turns out that it takes and average of 3 milliseconds to parse a single line of BIR
language. This means that, in a system where it will be rare that there will be more
than 1000 lines of BIR code, the parser does not really represent a bottleneck, given
the fact that we are using Rosette to generate new code, and continuously
calling the solver. According to this number, and the fact that the library we
would be using would not be as optimized as the Python one (`pyparsing`), the performance
gain would be minimal, if not negative. And all would cost huge amount of time for
developing and debugging a new solution from scratch.

All the benchmark code can be found on GitHub (\cite{benchmark}).

\subsection[Real extensibility]{Cons 4: Real extensibility}
Another point that I made about switching to Rust was the eventual extensibility
of the produced code, allowing for future new parser and eventual. If we created
a `parser` trait, then any eventual additional language could have been the same
trait, and therefore, once tested there was no need in the actual code to handle
the new addition. This is a really useful feature, if you ever use it.

As we thought, the scope of this thesis was creating a new pipeline to handle the
BIR language, that was more robust and researched for representing a ISA of a
real CPU. How many other times there will be the need to implement a new language
for the ISA? Probably never. There are not even much other candidate to do so,
and BIR seems to be, as today, the best. Therefore, the extensibility of the
codebase was not a real concern, and it was not worth the effort to switch to Rust
for this reason.

\subsection[Development speed]{Cons 5: Development speed}
Another fact is that, compared to Python, Rust is way slower to develop. This is
because of the borrow checker, that is a really powerful tool, but it can be a
pain to work with. In this case, development performance is more important, since
this is a research project. So, beigng able to do things and destroy afterwards,
without spending astonishing amount of times was a huge win, and for this Python
was just the best. Plus, on the team there was no high expertise in Rust. This
would have meant technical debt to be paid in the future, and hufe amout of time
spent on learning the language. Given the type of project in itself, this was not
worth the effort.

\subsection[Safety]{Cons 6: Safety}
One last point is on safety. This could be a deal-breaker for some cases, where the
codebase is really critical. But in this case, the codebase is not critical, and
it is used to synthesize contracts from the CPU, from a local computer. This
means no huge throughput, not real race conditions, and probably not real issues
at all with memory management.

Ultimately, the safety that Rust would have naturally given to the codebase, was
not something that was stricly required, nor even much useful.